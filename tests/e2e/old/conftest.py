import logging
import os
import uuid
from datetime import (
    datetime,
    timezone,
)

from neptune_scale import Run
from pytest import fixture

from neptune_fetcher import ReadOnlyRun

# NOTE
# The fixtures below assume that we're testing on a "many metrics" project
# populated using the tests/populate_projects.py script, and that the project
# contains ONLY data generated by the script.
# If this is not the case, the tests will fail, as there are hard assumptions
# around names and values of the metadata.


@fixture(scope="session", autouse=True)
def cleanup_logging_handlers():
    """Remove all logging handlers after each test session, to avoid
    messy errors from the `logging` module.

    The errors happen because `Run` installs `Run.close` as an `atexit` handler.
    The method logs some messages. The output is captured by pytest, which closes
    its logging handler early, causing "ValueError: I/O operation on closed file."
    """

    try:
        yield
    finally:
        logger = logging.getLogger("neptune")
        logger.handlers.clear()


@fixture
def all_run_ids():
    return sorted([f"id-run-{i + 1}" for i in range(6)])


@fixture
def all_experiment_ids():
    return sorted([f"id-exp-{i + 1}" for i in range(6)])


@fixture
def all_experiment_names():
    return sorted([f"exp{i + 1}" for i in range(6)])


@fixture
def sys_columns_set(sys_columns):
    return set(sys_columns)


@fixture
def sys_columns():
    return ["sys/id", "sys/name", "sys/custom_run_id"]


@fixture
def id_to_name():
    """Expected sys/custom_run_id -> sys/name"""
    d = {f"id-run-{num}": "" for num in range(1, 7)}
    d |= {f"id-exp-{num}": f"exp{num}" for num in range(1, 7)}

    return d


class SyncRun(Run):
    """A neptune_scale.Run instance that waits for processing to complete
    after each logging method call. This is useful for e2e tests, where we
    usually want to wait for the data to be available before fetching it."""

    def log(self, *args, **kwargs):
        result = super().log(*args, **kwargs)
        self.wait_for_processing()
        return result

    def log_configs(self, *args, **kwargs):
        result = super().log_configs(*args, **kwargs)
        self.wait_for_processing()
        return result

    def log_metrics(self, *args, **kwargs):
        result = super().log_metrics(*args, **kwargs)
        self.wait_for_processing()
        return result

    def add_tags(self, *args, **kwargs):
        result = super().add_tags(*args, **kwargs)
        self.wait_for_processing()
        return result

    def remove_tags(self, *args, **kwargs):
        result = super().remove_tags(*args, **kwargs)
        self.wait_for_processing()
        return result


@fixture(scope="module")
def run_init_kwargs(project):
    """Arguments to initialize a neptune_scale.Run instance"""

    # TODO: if a test fails the run could be left in an indefinite state
    #       Maybe we should just have it scoped 'function' and require passing
    #       an existing run id
    kwargs = {"project": project.project_identifier}
    run_id = os.getenv("NEPTUNE_E2E_CUSTOM_RUN_ID")
    if run_id is None:
        run_id = str(uuid.uuid4())
        kwargs["experiment_name"] = "pye2e-fetcher"
    else:
        kwargs["resume"] = True

    kwargs["run_id"] = run_id

    return kwargs


@fixture(scope="module")
def run(project, run_init_kwargs):
    """Plain neptune_scale.Run instance. We're scoping it to "module", as it seems to be a
    good compromise, mostly because of execution time."""

    run = Run(**run_init_kwargs)
    run.log_configs({"test_start_time": datetime.now(timezone.utc)})
    run.wait_for_processing()

    return run


@fixture(scope="module")
def sync_run(project, run_init_kwargs):
    """Blocking run for logging data"""
    return SyncRun(project=run_init_kwargs["project"], run_id=run_init_kwargs["run_id"], resume=True)


@fixture
def ro_run(project, run, run_init_kwargs):
    """ReadOnlyRun pointing to the same run as the neptune_scale.Run"""
    return ReadOnlyRun(read_only_project=project, custom_id=run_init_kwargs["run_id"])


# TODO: chyba niepotrzebne
def pytest_set_filtered_exceptions() -> list[type[BaseException]]:
    class DoNotFilterAnythingMarker(Exception):
        pass

    return [DoNotFilterAnythingMarker]
